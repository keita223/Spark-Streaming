# Compte Rendu - TP4 Spark Streaming

## Auteur
R√©alis√© dans le cadre du cours de Data Streaming - M2 IASD

Date: 18 D√©cembre 2025

---

## Objectifs du TP

‚úÖ Disposer d'un pipeline Kafka ‚Üí Spark Streaming fonctionnel
‚úÖ Comprendre Spark Streaming dans ses grandes lignes
‚úÖ √ätre √† l'aise avec la lecture et la transformation des donn√©es en flux continu

---

## Architecture du Projet

```
Producteur Python (Windows)
    ‚Üì
    ‚Üí localhost:9092 (EXTERNAL)
    ‚Üì
KAFKA (Docker)
    ‚Üí kafka:29092 (INTERNAL)
    ‚Üì
SPARK STREAMING (Docker)
    ‚Üì
CONSOLE OUTPUT (filtr√©)
```

### Configuration Kafka - Double Listener

Pour permettre la communication entre:
- **Producteur Python (Windows)** ‚Üí `localhost:9092`
- **Spark (Docker)** ‚Üí `kafka:29092`

```yaml
KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
```

---

## Commandes Importantes

### 1. D√©marrage de l'Infrastructure

```bash
# D√©marrer les conteneurs Docker (Zookeeper, Kafka, Spark)
docker-compose up -d

# V√©rifier que les conteneurs sont en cours d'ex√©cution
docker ps

# Arr√™ter les conteneurs
docker-compose down
```

### 2. Gestion de Kafka

```bash
# Lister les topics Kafka
docker exec kafka kafka-topics --list --bootstrap-server kafka:29092

# Cr√©er un topic
docker exec kafka kafka-topics --create --topic test-topic --bootstrap-server kafka:29092 --partitions 1 --replication-factor 1

# Supprimer un topic
docker exec kafka kafka-topics --delete --topic test-topic --bootstrap-server kafka:29092

# Voir les d√©tails d'un topic
docker exec kafka kafka-topics --describe --topic test-topic --bootstrap-server kafka:29092
```

### 3. Lancement du Pipeline

**Terminal 1 - Producteur Kafka:**
```bash
python kafka/producer.py
```

**Terminal 2 - Spark Streaming:**
```bash
docker exec spark /opt/spark/bin/spark-submit \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0 \
  /app/spark_kafka_stream.py
```

**Alternative - Entrer dans le conteneur Spark d'abord:**
```bash
# √âtape 1: Entrer dans le conteneur
docker exec -it spark bash

# √âtape 2: Lancer Spark
/opt/spark/bin/spark-submit \
  --conf spark.jars.ivy=/tmp/.ivy2 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0 \
  /app/spark_kafka_stream.py
```

### 4. Commandes de Debugging

```bash
# Voir les logs d'un conteneur
docker logs kafka
docker logs spark
docker logs zookeeper

# Voir les logs en temps r√©el
docker logs -f kafka

# V√©rifier l'√©tat du r√©seau Docker
docker network ls
docker network inspect spark-streaming_default

# Tester la connectivit√© entre conteneurs
docker exec spark ping kafka
```

---

## Erreurs Courantes et Solutions

### ‚ùå Erreur 1: PySpark ne fonctionne pas sur Windows

**Erreur:**
```
AttributeError: module 'socketserver' has no attribute 'UnixStreamServer'
```

**Cause:**
PySpark utilise des d√©pendances Unix qui ne sont pas disponibles sur Windows.

**Solution:**
Ex√©cuter Spark **dans le conteneur Docker** au lieu de Windows:
```bash
docker exec spark /opt/spark/bin/spark-submit ...
```

---

### ‚ùå Erreur 2: Version incompatible du package Kafka

**Erreur:**
```
NoClassDefFoundError: org/apache/spark/internal/LogKeys$NUM_RETRY
```

**Cause:**
Utilisation de `spark-sql-kafka-0-10_2.12:3.5.0` avec Spark 4.1.0

**Solution:**
Utiliser la version compatible:
```python
.config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0")
```

**R√®gle:**
- Spark 4.1.0 utilise **Scala 2.13** ‚Üí package `_2.13:4.1.0`
- Format: `spark-sql-kafka-0-10_<scala-version>:<spark-version>`

---

### ‚ùå Erreur 3: Connection to node -1 (localhost/127.0.0.1:9092) could not be established

**Erreur:**
```
Connection to node 1 (localhost/127.0.0.1:9092) could not be established
```

**Cause:**
Kafka annonce `localhost:9092` mais Spark (dans Docker) cherche sur son propre conteneur.

**Solution 1 - Double Listener (RECOMMAND√â):**
```yaml
KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
```
- Producteur (Windows) ‚Üí `localhost:9092`
- Spark (Docker) ‚Üí `kafka:29092`

**Solution 2 - Tout dans Docker:**
```yaml
KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
```
Mais le producteur doit aussi tourner dans Docker.

---

### ‚ùå Erreur 4: Permission denied - Ivy cache

**Erreur:**
```
FileNotFoundException: /nonexistent/.ivy2.5.2/cache/...
```

**Cause:**
Spark essaie d'√©crire dans un r√©pertoire non accessible.

**Solution:**
Sp√©cifier un r√©pertoire accessible:
```bash
--conf spark.jars.ivy=/tmp/.ivy2
```

---

### ‚ùå Erreur 5: Kafka timeout - Failed to update metadata

**Erreur:**
```
KafkaTimeoutError: Failed to update metadata after 60.0 secs
```

**Cause:**
Le producteur/consommateur ne peut pas se connecter √† Kafka.

**Solutions:**
1. V√©rifier que Kafka est d√©marr√©: `docker ps`
2. V√©rifier que le topic existe: `docker exec kafka kafka-topics --list --bootstrap-server kafka:29092`
3. V√©rifier la configuration des listeners dans docker-compose.yml
4. Attendre 10-15 secondes apr√®s `docker-compose up -d` avant de lancer les applications

---

## Structure du Code

### spark_kafka_stream.py

```python
# Configuration essentielle
spark = SparkSession.builder \
    .appName("KafkaSparkStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.13:4.1.0") \
    .master("local[*]") \
    .getOrCreate()

# Connexion √† Kafka (depuis Docker)
kafka_bootstrap_servers = "kafka:29092"
kafka_topic = "test-topic"

# Lecture du stream
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_bootstrap_servers) \
    .option("subscribe", kafka_topic) \
    .option("startingOffsets", "earliest") \
    .load()

# Transformation (filtrage)
df_filtered = df_string.select("value", "timestamp") \
    .filter(col("value").contains("important"))

# √âcriture du r√©sultat
query = df_filtered.writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", False) \
    .start()

# Attendre la fin
query.awaitTermination()
```

### kafka/producer.py

```python
producer = KafkaProducer(
    bootstrap_servers='localhost:9092',  # EXTERNAL listener
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

messages = [
    "This is an important message",
    "This is a regular message",
    "Another important update",
    "Just a normal event",
    "Critical important alert",
    "Random data here",
    "Important notification received"
]

# Envoi continu de messages
while True:
    message = random.choice(messages)
    producer.send('test-topic', value=message)
    time.sleep(2)
```

---

## Questions de R√©flexion

### 1. En quoi Spark Streaming diff√®re-t-il d'un consommateur Kafka ?

**Consommateur Kafka classique:**
- Lit les messages un par un ou par petits lots
- Traitement simple et direct
- Logique de traitement √©crite manuellement
- G√®re manuellement les offsets et la parall√©lisation

**Spark Streaming:**
- Traite les donn√©es par **micro-batches** (petits lots toutes les quelques secondes)
- Fournit un **DataFrame API** pour des transformations complexes (filter, select, groupBy, etc.)
- **Parall√©lise automatiquement** le traitement sur plusieurs c≈ìurs/machines
- Offre des garanties de tol√©rance aux pannes (peut reprendre apr√®s un crash)
- Permet des op√©rations avanc√©es: agr√©gations, fen√™tres temporelles, jointures

**En r√©sum√©:** Un consommateur Kafka lit des messages, Spark Streaming traite des flux de donn√©es avec des op√©rations complexes.

---

### 2. Quel r√¥le joue Kafka dans ce processus ?

Kafka agit comme un **tampon de messages distribu√©** (message buffer):

- **Stockage temporaire:** Garde les messages pendant une dur√©e configur√©e (retention)
- **D√©couplage:** Les producteurs et consommateurs n'ont pas besoin d'√™tre synchronis√©s
- **R√©silience:** Si Spark tombe en panne, les messages restent dans Kafka
- **Rejouabilit√©:** Spark peut relire les messages depuis le d√©but (`startingOffsets: "earliest"`)
- **Scalabilit√©:** Kafka peut g√©rer des millions de messages/seconde

**Analogie:** Kafka = la file d'attente dans un restaurant, Spark = le chef qui prend les commandes et les traite.

---

### 3. Quel r√¥le joue Spark ?

Spark est le **moteur de traitement et d'analyse** des donn√©es:

- **Transformation:** Applique des filtres, agr√©gations, calculs sur les donn√©es
- **Parall√©lisation:** Distribue le calcul sur plusieurs machines si n√©cessaire
- **√âtat:** Peut maintenir un √©tat entre les batches (compteurs, agr√©gations)
- **Sortie:** √âcrit les r√©sultats vers diff√©rentes destinations (console, fichiers, bases de donn√©es)

**Exemple concret du TP:**
```python
# Spark lit depuis Kafka
df = spark.readStream.format("kafka").load()

# Spark transforme les donn√©es
df_filtered = df.filter(col("value").contains("important"))

# Spark √©crit le r√©sultat
query = df_filtered.writeStream.format("console").start()
```

**Analogie:** Spark = usine de traitement qui transforme les mati√®res premi√®res (messages Kafka) en produits finis (r√©sultats).

---

### 4. Pourquoi s√©parer le stockage des donn√©es (Kafka) du calcul (Spark) ?

Cette s√©paration suit le principe **"Separation of Concerns"** et apporte plusieurs avantages:

**a) Scalabilit√© ind√©pendante:**
- Vous pouvez ajouter plus de brokers Kafka sans toucher √† Spark
- Vous pouvez ajouter plus de workers Spark sans toucher √† Kafka

**b) Flexibilit√©:**
- Plusieurs applications peuvent lire le m√™me topic Kafka (Spark, Python consumer, autre syst√®me)
- Vous pouvez changer votre logique Spark sans perdre les donn√©es Kafka

**c) R√©silience:**
- Si Spark tombe, Kafka continue √† recevoir des messages
- Si Kafka red√©marre, Spark peut reprendre l√† o√π il s'√©tait arr√™t√©

**d) Sp√©cialisation:**
- Kafka est optimis√© pour le stockage et la distribution de messages
- Spark est optimis√© pour le calcul et les transformations complexes

**Exemple concret dans le TP:**
- Le `producer.py` continue d'envoyer des messages m√™me si Spark n'est pas lanc√©
- Quand on d√©marre Spark avec `startingOffsets: "earliest"`, il peut lire tous les messages depuis le d√©but
- On peut arr√™ter/red√©marrer Spark sans perdre de donn√©es

---

## Concepts Cl√©s Appris

### 1. Streaming DataFrame
Un DataFrame qui repr√©sente un flux continu de donn√©es. Il a le m√™me API qu'un DataFrame batch, mais les donn√©es arrivent en continu.

### 2. Micro-batch
Spark Streaming traite les donn√©es par **petits lots** (par d√©faut toutes les quelques secondes), pas message par message.

### 3. Schema Kafka
Les messages Kafka apparaissent dans Spark avec cette structure:
```
root
 |-- key: binary (nullable = true)
 |-- value: binary (nullable = true)
 |-- topic: string (nullable = true)
 |-- partition: integer (nullable = true)
 |-- offset: long (nullable = true)
 |-- timestamp: timestamp (nullable = true)
 |-- timestampType: integer (nullable = true)
```

### 4. Output Modes
- **append:** Ajoute seulement les nouvelles lignes (utilis√© dans le TP)
- **complete:** R√©√©met tout le r√©sultat √† chaque batch
- **update:** √âmet seulement les lignes modifi√©es

### 5. Checkpointing
Spark peut sauvegarder son √©tat pour reprendre apr√®s un crash (non utilis√© dans ce TP mais important en production).

---

## Bonnes Pratiques

### Configuration Docker

‚úÖ **Utiliser des listeners s√©par√©s pour Kafka:**
```yaml
KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:29092,EXTERNAL://localhost:9092
```

‚úÖ **Monter le code en volume:**
```yaml
volumes:
  - ./:/app
```
Permet de modifier le code sans reconstruire l'image.

‚úÖ **Attendre que Kafka soit pr√™t:**
```bash
sleep 15 && docker exec kafka kafka-topics --list ...
```

### Configuration Spark

‚úÖ **Sp√©cifier le cache Ivy:**
```bash
--conf spark.jars.ivy=/tmp/.ivy2
```

‚úÖ **Utiliser la bonne version du package:**
```
spark-sql-kafka-0-10_2.13:4.1.0  # Pour Spark 4.1.0
```

‚úÖ **D√©finir le log level:**
```python
spark.sparkContext.setLogLevel("WARN")
```

### D√©veloppement

‚úÖ **Tester d'abord la connexion Kafka:**
```bash
docker exec kafka kafka-topics --list --bootstrap-server kafka:29092
```

‚úÖ **Utiliser startingOffsets: "earliest" pour le d√©veloppement:**
```python
.option("startingOffsets", "earliest")
```
Permet de rejouer tous les messages.

‚úÖ **V√©rifier les logs en cas d'erreur:**
```bash
docker logs kafka
docker logs spark
```

---

## R√©sultats Obtenus

### Output du Producteur
```
üì§ Starting to send messages to topic: test-topic
‚úÖ Sent message 1: This is an important message
‚úÖ Sent message 2: Just a normal event
‚úÖ Sent message 3: Another important update
‚úÖ Sent message 4: This is a regular message
‚úÖ Sent message 5: Critical important alert
```

### Output de Spark (filtr√©)
```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------------------------+-------------------+
|value                      |timestamp          |
+---------------------------+-------------------+
|This is an important message|2025-12-18 21:30:15|
|Another important update    |2025-12-18 21:30:19|
|Critical important alert    |2025-12-18 21:30:23|
+---------------------------+-------------------+
```

Seuls les messages contenant "important" sont affich√©s! ‚úÖ

---

## Am√©liorations Possibles

### 1. Agr√©gations
Compter le nombre de messages "important" par fen√™tre de temps:
```python
from pyspark.sql.functions import window

df_windowed = df_filtered.groupBy(
    window("timestamp", "1 minute")
).count()
```

### 2. Parsing JSON
Si les messages sont en JSON:
```python
from pyspark.sql.functions import from_json, schema_of_json

schema = schema_of_json('{"user": "alice", "action": "click"}')
df_json = df.select(from_json(col("value"), schema).alias("data"))
```

### 3. √âcriture vers fichiers
```python
query = df_filtered.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "/app/output") \
    .option("checkpointLocation", "/app/checkpoint") \
    .start()
```

### 4. Multiple topics
```python
.option("subscribe", "topic1,topic2,topic3")
```

---

## Conclusion

Ce TP a permis de:

‚úÖ Construire un pipeline Kafka ‚Üí Spark Streaming complet
‚úÖ Comprendre les diff√©rences entre stockage (Kafka) et traitement (Spark)
‚úÖ Ma√Ætriser la configuration Docker pour la communication inter-conteneurs
‚úÖ R√©soudre les probl√®mes courants de compatibilit√© et de connectivit√©
‚úÖ Appliquer des transformations sur des flux de donn√©es en temps r√©el

**Points cl√©s √† retenir:**
- Spark Streaming utilise le m√™me API que Spark batch
- Kafka et Spark sont deux syst√®mes compl√©mentaires
- La configuration r√©seau est cruciale pour Docker
- Les micro-batches permettent un bon compromis entre latence et d√©bit

---

## R√©f√©rences

- [Apache Spark Structured Streaming](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)
- [Spark Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)
- [Confluent Kafka Docker](https://docs.confluent.io/platform/current/installation/docker/config-reference.html)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)

---

**Auteur:** R√©alis√© dans le cadre du TP4 - M2 IASD
**Date:** 18 D√©cembre 2025
